{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89901fc1",
   "metadata": {},
   "source": [
    "# MNIST Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b7d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from si.supervised.nn import NN, Dense, Adam, LeakyReLU, Tanh, BatchNormalization\n",
    "from si.data import Dataset, LabelEncoder, OneHotEncoder\n",
    "from si.util import minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38467ca1",
   "metadata": {},
   "source": [
    "## Build the autoencoder\n",
    "\n",
    "An Autoencoder with deep fully-connected neural nets.\n",
    "Training Data: MNIST Handwritten Digits (28x28 images)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94098bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.img_dim = self.img_rows * self.img_cols\n",
    "        self.latent_dim = 128 # The dimension of the data embedding\n",
    "\n",
    "        loss_function = 'RMSE'\n",
    "        optimizer = Adam(learning_rate=0.0005, b1=0.5)\n",
    "        \n",
    "\n",
    "        self.encoder = self.build_encoder(optimizer, loss_function)\n",
    "        self.decoder = self.build_decoder(optimizer, loss_function)\n",
    "\n",
    "        self.autoencoder = NN(optimizer=optimizer, loss=loss_function)\n",
    "        self.autoencoder.layers.extend(self.encoder.layers)\n",
    "        self.autoencoder.layers.extend(self.decoder.layers)\n",
    "\n",
    "        \n",
    "    def build_encoder(self, optimizer, loss_function):\n",
    "\n",
    "        encoder = NN(optimizer=optimizer, loss=loss_function)\n",
    "        encoder.add(Dense(self.img_dim,512))\n",
    "        encoder.add(LeakyReLU())\n",
    "        encoder.add(BatchNormalization(512, momentum=0.8))\n",
    "        encoder.add(Dense(512,256))\n",
    "        encoder.add(LeakyReLU())\n",
    "        encoder.add(BatchNormalization(256,momentum=0.8))\n",
    "        encoder.add(Dense(256,self.latent_dim))\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def build_decoder(self, optimizer, loss_function):\n",
    "\n",
    "        decoder = NN(optimizer=optimizer, loss=loss_function)\n",
    "        decoder.add(Dense(self.latent_dim, 256))\n",
    "        decoder.add(LeakyReLU())\n",
    "        decoder.add(BatchNormalization(256, momentum=0.8))\n",
    "        decoder.add(Dense(256,512))\n",
    "        decoder.add(LeakyReLU())\n",
    "        decoder.add(BatchNormalization(512, momentum=0.8))\n",
    "        decoder.add(Dense(512,self.img_dim))\n",
    "        decoder.add(Tanh())\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    def train(self, dataset, epochs, batch_size=128):\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            \n",
    "            x_orig = []\n",
    "            x_pred = []\n",
    "            \n",
    "            for batch in minibatch(dataset.X, batchsize=batch_size):\n",
    "                output_batch = np.copy(batch)\n",
    "                 \n",
    "                for layer in self.autoencoder.layers:\n",
    "                    output_batch = layer.forward(output_batch)\n",
    "    \n",
    "                error = self.autoencoder.loss_prime(batch, output_batch)\n",
    "                \n",
    "                for layer in reversed(self.autoencoder.layers):\n",
    "                    error = layer.backward(error)\n",
    "\n",
    "                x_orig.append(batch)\n",
    "                x_pred.append(output_batch)\n",
    "            \n",
    "            orig = np.concatenate(x_orig)\n",
    "            pred = np.concatenate(x_pred)\n",
    "            \n",
    "            # compute the loss\n",
    "            err = self.autoencoder.loss(orig,pred)\n",
    "\n",
    "            s = f\"epoch {epoch}/{epochs} loss={err}\"\n",
    "            print(s, end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740dd0d",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d962681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(sample_size=None):\n",
    "    DIR = os.path.dirname(os.path.realpath('.'))\n",
    "    filename = os.path.join(DIR, 'datasets/mnist.pkl.gz')\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    data = pickle.load(f, encoding='bytes')\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    if sample_size:\n",
    "        return (Dataset(x_train[:sample_size],\n",
    "                y_train[:sample_size]),Dataset(x_test,y_test))\n",
    "    else:\n",
    "        return Dataset(x_train,y_train),Dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e43f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = load_mnist(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7a0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    # reshape and normalize input data\n",
    "    dataset.X = dataset.X.reshape(dataset.X.shape[0], 28*28)\n",
    "    dataset.X = dataset.X.astype('float32')\n",
    "    dataset.X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4888af",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c34162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000/1000 loss=0.19775809812486586\r"
     ]
    }
   ],
   "source": [
    "ae = Autoencoder()\n",
    "ae.train(train,1000,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbb022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb91e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
